"""
SQLç”Ÿæˆç³»ç»Ÿ - å‡½æ•°å¼ç¼–ç¨‹ç‰ˆæœ¬
æ•´åˆäº†å›¾ç³»ç»Ÿå’Œä¸»ç¨‹åºåŠŸèƒ½
"""

import logging
import argparse
from pathlib import Path
import time
import sys
import json
from datetime import datetime
from typing import Dict, Any, List, Tuple
import jsonlines
import threading
import tempfile
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import os

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from Communicate import SystemState
from BuildAgentSystem import build_agent_system

# å¯¼å…¥è¿æ¥æ± æ¨¡å—
try:
    from utils.SnowflakeConnectionPool import get_global_pool, close_global_pool, get_pool_stats
    HAS_CONNECTION_POOL = True
except ImportError:
    HAS_CONNECTION_POOL = False

# å…¨å±€é…ç½®
logger = logging.getLogger(__name__)

# å…¨å±€é”ç”¨äºçº¿ç¨‹å®‰å…¨
file_lock = threading.Lock()
log_lock = threading.Lock()

def create_thread_workspace(thread_id: str, base_temp_dir: Path) -> Path:
    """
    ä¸ºçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶å·¥ä½œç›®å½•
    
    Args:
        thread_id: çº¿ç¨‹ID
        base_temp_dir: åŸºç¡€ä¸´æ—¶ç›®å½•
        
    Returns:
        çº¿ç¨‹å·¥ä½œç›®å½•è·¯å¾„
    """
    thread_dir = base_temp_dir / f"thread_{thread_id}"
    thread_dir.mkdir(parents=True, exist_ok=True)
    return thread_dir

def process_single_query_with_stats(
    item: Dict,
    results_dir: Path,
    base_temp_dir: Path,
    timeout_seconds: int = 300
) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæŸ¥è¯¢å¹¶è¿”å›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        item: æŸ¥è¯¢é¡¹
        results_dir: ç»“æœè¾“å‡ºç›®å½•
        base_temp_dir: åŸºç¡€ä¸´æ—¶ç›®å½•
        timeout_seconds: è¶…æ—¶æ—¶é—´(ç§’)
        
    Returns:
        åŒ…å«è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    thread_id = threading.current_thread().ident
    instance_id = item.get("instance_id", "unknown")
    
    try:
        # åˆ›å»ºçº¿ç¨‹ç‹¬ç«‹å·¥ä½œç›®å½•
        thread_workspace = create_thread_workspace(str(thread_id), base_temp_dir)
        
        with log_lock:
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {instance_id}")
        
        # æå–æŸ¥è¯¢ä¿¡æ¯
        instruction = item.get("instruction", "")
        db_id = item.get("db_id", "")
        
        if not instruction or not db_id:
            raise ValueError(f"æŸ¥è¯¢ä¿¡æ¯ä¸å®Œæ•´: instruction={bool(instruction)}, db_id={bool(db_id)}")
        
        start_time = time.time()
        
        try:
            # æ„å»ºAgentç³»ç»Ÿå›¾
            graph = build_agent_system()
            
            # åˆå§‹çŠ¶æ€
            initial_state: SystemState = {
                "user_query": instruction,
                "database_id": db_id,
                "schema_info": {},
                "generated_sql": "",
                "execution_result": {},
                "step": "start",
                "iteration": 0,
                "final_sql": "",
                "final_result": [],
                "error_message": "",
                "is_completed": False
            }
            
            # è¿è¡Œå›¾
            config = {"configurable": {"thread_id": f"sql_session_{thread_id}"}}
            result = graph.invoke(initial_state, config)
            
            elapsed_time = time.time() - start_time
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if elapsed_time > timeout_seconds:
                raise TimeoutError(f"SQLç”Ÿæˆè¶…æ—¶: {elapsed_time:.2f}ç§’")
            
            result['execution_time'] = elapsed_time
            
            # æ ¹æ®æ–°çš„æˆåŠŸå®šä¹‰ï¼šåªæœ‰è¿”å›æ•°æ®å¤§äº0æ¡æ‰ç®—æˆåŠŸ
            has_data = bool(result.get('final_result', [])) and len(result.get('final_result', [])) > 0
            is_completed = result.get('is_completed', False)
            success = is_completed and has_data  # å¿…é¡»å®Œæˆä¸”æœ‰æ•°æ®æ‰ç®—æˆåŠŸ
            
            error_msg = result.get('error_message', '')
            iterations = result.get('iteration', 0)
            
            # æ›´æ–°çŠ¶æ€åˆ†ç±»é€»è¾‘
            if success:
                status = 'success_with_data'
            elif is_completed and not has_data:
                status = 'completed_no_data'  # å®Œæˆä½†æ— æ•°æ®ï¼ŒæŒ‰æ–°å®šä¹‰ç®—å¤±è´¥
            else:
                status = 'failed'
            
            # å¦‚æœå®Œæˆä½†æ— æ•°æ®ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯
            if is_completed and not has_data and not error_msg:
                error_msg = "æŸ¥è¯¢å®Œæˆä½†æœªè¿”å›æ•°æ®"
            
            # ä¿å­˜SQLæ–‡ä»¶
            sql_file = save_sql_to_file(
                result=result,
                instance_id=instance_id,
                query=instruction,
                database_id=db_id,
                results_dir=results_dir
            )
            
            with log_lock:
                logger.info(f"å®Œæˆå¤„ç†æŸ¥è¯¢: {instance_id}, è€—æ—¶: {elapsed_time:.2f}ç§’, æˆåŠŸ: {success}")
            
            return {
                'instance_id': instance_id,
                'success': success,
                'error_msg': error_msg if not success else "",
                'iterations': iterations,
                'has_data': has_data,
                'status': status,
                'elapsed_time': elapsed_time,
                'final_sql': result.get('final_sql', ''),
                'final_result': result.get('final_result', [])
            }
            
        except Exception as graph_error:
            error_msg = f"å›¾æ‰§è¡Œå¤±è´¥: {graph_error}"
            with log_lock:
                logger.error(f"æŸ¥è¯¢ {instance_id} {error_msg}")
            return {
                'instance_id': instance_id,
                'success': False,
                'error_msg': error_msg,
                'iterations': 0,
                'has_data': False,
                'status': 'failed',
                'elapsed_time': time.time() - start_time,
                'final_sql': '',
                'final_result': []
            }
        
    except TimeoutError as e:
        error_msg = f"è¶…æ—¶é”™è¯¯: {str(e)}"
        with log_lock:
            logger.error(f"æŸ¥è¯¢ {instance_id} å¤„ç†è¶…æ—¶: {error_msg}")
        return {
            'instance_id': instance_id,
            'success': False,
            'error_msg': error_msg,
            'iterations': 0,
            'has_data': False,
            'status': 'failed',
            'elapsed_time': timeout_seconds
        }
        
    except Exception as e:
        error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
        with log_lock:
            logger.error(f"æŸ¥è¯¢ {instance_id} å¤„ç†å¤±è´¥: {error_msg}")
        return {
            'instance_id': instance_id,
            'success': False,
            'error_msg': error_msg,
            'iterations': 0,
            'has_data': False,
            'status': 'failed',
            'elapsed_time': 0
        }
    
    finally:
        # æ¸…ç†çº¿ç¨‹å·¥ä½œç›®å½•
        try:
            if 'thread_workspace' in locals() and thread_workspace.exists():
                shutil.rmtree(thread_workspace)
        except Exception as e:
            with log_lock:
                logger.warning(f"æ¸…ç†çº¿ç¨‹å·¥ä½œç›®å½•å¤±è´¥: {e}")

# ===== AgentèŠ‚ç‚¹å‡½æ•°å·²ç§»åŠ¨åˆ°BuildAgentSystem.pyä¸­ =====



# ===== å·¥å…·å‡½æ•° =====

def setup_logging() -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('sql_generation.log'),
            logging.StreamHandler()
        ]
    )

def save_sql_to_file(
    result: Dict[str, Any], 
    instance_id: str, 
    query: str, 
    database_id: str, 
    results_dir: Path
) -> str:
    """
    å°†SQLæŸ¥è¯¢ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        result: ç³»ç»Ÿæ‰§è¡Œç»“æœ
        instance_id: æŸ¥è¯¢å®ä¾‹ID
        query: åŸå§‹æŸ¥è¯¢
        database_id: æ•°æ®åº“ID
        results_dir: ç»“æœä¿å­˜ç›®å½•
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        output_file = results_dir / f"{instance_id}.sql"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"-- Instance ID: {instance_id}\n")
            f.write(f"-- Query: {query}\n")
            f.write(f"-- Database: {database_id}\n")
            f.write(f"-- Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"-- Execution time: {result.get('execution_time', 0):.2f}s\n")
            f.write(f"-- Success: {result.get('success', False)} (æˆåŠŸå®šä¹‰ï¼šè¿”å›æ•°æ®>0æ¡)\n")
            f.write(f"-- Iterations: {result.get('iterations', 0)}\n")
            
            if result.get('error_message'):
                f.write(f"-- Error: {result['error_message']}\n")
            
            f.write("\n")
            f.write(result.get('final_sql', '-- No SQL generated'))
        
        return str(output_file)
        
    except Exception as e:
        logger.error(f"ä¿å­˜SQLæ–‡ä»¶å¤±è´¥ {instance_id}: {e}")
        return ""

def load_queries(input_file: Path) -> List[Dict]:
    """
    åŠ è½½æŸ¥è¯¢æ•°æ®
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        
    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    queries = []
    
    try:
        with jsonlines.open(input_file) as reader:
            for item in reader:
                queries.append(item)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(queries)} ä¸ªæŸ¥è¯¢")
        return queries
        
    except Exception as e:
        logger.error(f"åŠ è½½æŸ¥è¯¢æ–‡ä»¶å¤±è´¥ {input_file}: {e}")
        return []

def create_timestamped_directory(base_path: Path, prefix: str = "results") -> Path:
    """
    åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç›®å½•
    
    Args:
        base_path: åŸºç¡€è·¯å¾„
        prefix: ç›®å½•å‰ç¼€
        
    Returns:
        åˆ›å»ºçš„ç›®å½•è·¯å¾„
    """
    # ç”ŸæˆçŸ­æ—¶é—´æˆ³ (æ ¼å¼: YYYYMMDD_HHMMSS)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = base_path / f"{prefix}_{timestamp}"
    results_dir.mkdir(exist_ok=True)
    
    logger.info(f"åˆ›å»ºç»“æœç›®å½•: {results_dir}")
    return results_dir

def process_batch_queries(queries: List[Dict], results_dir: Path, max_workers: int = None, timeout_seconds: int = 300) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†æŸ¥è¯¢
    
    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        results_dir: ç»“æœä¿å­˜ç›®å½•
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        timeout_seconds: å•ä¸ªæŸ¥è¯¢è¶…æ—¶æ—¶é—´
        
    Returns:
        å¤„ç†ç»“æœæ‘˜è¦
    """
    total_queries = len(queries)
    success_count = 0  # æˆåŠŸï¼šæœ‰æ•°æ®è¿”å›
    failed_count = 0   # å¤±è´¥ï¼šæ— æ•°æ®è¿”å›æˆ–æ‰§è¡Œå¤±è´¥
    completed_no_data_count = 0  # å®Œæˆä½†æ— æ•°æ®çš„æ•°é‡ï¼ˆç®—ä½œå¤±è´¥ï¼Œä½†å•ç‹¬ç»Ÿè®¡ï¼‰
    total_iterations = 0
    failed_items = []
    instance_results = []  # è®°å½•æ¯ä¸ªinstanceçš„ç»“æœ
    
    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {total_queries} ä¸ªæŸ¥è¯¢...")
    logger.info(f"æˆåŠŸå®šä¹‰ï¼šåœ¨è§„å®šæ¬¡æ•°å†…ç”Ÿæˆå‡ºå¯ä»¥æŸ¥è¯¢åˆ°å¤§äºä¸€æ¡æ•°æ®çš„ç»“æœ")
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºåŸºç¡€ä¸´æ—¶ç›®å½•
    base_temp_dir = Path(tempfile.mkdtemp(prefix="sql_gen_batch_"))
    
    try:
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
        if max_workers is None:
            max_workers = min(32, (os.cpu_count() or 1) + 4)
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        if HAS_CONNECTION_POOL:
            try:
                pool = get_global_pool(max_connections=max_workers)
                logger.info(f"è¿æ¥æ± å·²åˆå§‹åŒ–: max_connections={max_workers}")
            except Exception as e:
                logger.warning(f"è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹è¿æ¥æ–¹å¼: {e}")
            
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_query = {
                executor.submit(
                    process_single_query_with_stats,  # ä½¿ç”¨æ–°çš„å‡½æ•°
                    item,
                    results_dir,
                    base_temp_dir,
                    timeout_seconds
                ): item for item in queries
            }
            
            # ä½¿ç”¨as_completedå¼‚æ­¥æ”¶é›†ç»“æœ
            with tqdm(total=len(queries), desc="å¤„ç†è¿›åº¦") as pbar:
                for future in as_completed(future_to_query):
                    item = future_to_query[future]
                    
                    try:
                        result_data = future.result()
                        instance_id = result_data['instance_id']
                        success = result_data['success']
                        error_msg = result_data['error_msg']
                        iterations = result_data['iterations']
                        has_data = result_data['has_data']
                        status = result_data['status']
                        
                        # è®°å½•instanceç»“æœ
                        instance_result = {
                            "instance_id": instance_id,
                            "status": status,
                            "iterations": iterations,
                            "has_data": has_data
                        }
                        instance_results.append(instance_result)
                        
                        if success:
                            success_count += 1
                            total_iterations += iterations
                        else:
                            failed_count += 1
                            # å¦‚æœæ˜¯å®Œæˆä½†æ— æ•°æ®çš„æƒ…å†µï¼Œå•ç‹¬ç»Ÿè®¡
                            if status == 'completed_no_data':
                                completed_no_data_count += 1
                            
                            failed_items.append({
                                "instance_id": instance_id,
                                "error": error_msg,
                                "item": item
                            })
                        
                        pbar.update(1)
                        pbar.set_postfix({
                            "æˆåŠŸ": success_count,
                            "å¤±è´¥": failed_count,
                            "å¹³å‡è½®æ•°": f"{total_iterations/(success_count) if success_count > 0 else 0:.1f}"
                        })
                        
                    except Exception as e:
                        failed_count += 1
                        instance_id = item.get("instance_id", "unknown")
                        failed_items.append({
                            "instance_id": instance_id,
                            "error": f"Futureæ‰§è¡Œå¼‚å¸¸: {str(e)}",
                            "item": item
                        })
                        
                        # è®°å½•å¤±è´¥çš„instanceç»“æœ
                        instance_results.append({
                            "instance_id": instance_id,
                            "status": "failed",
                            "iterations": 0,
                            "has_data": False
                        })
                        
                        pbar.update(1)
                        pbar.set_postfix({
                            "æˆåŠŸ": success_count,
                            "å¤±è´¥": failed_count,
                            "å¹³å‡è½®æ•°": f"{total_iterations/(success_count) if success_count > 0 else 0:.1f}"
                        })
    
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        try:
            if base_temp_dir.exists():
                shutil.rmtree(base_temp_dir)
                logger.info("æ‰¹é‡å¤„ç†ä¸´æ—¶ç›®å½•å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"æ¸…ç†æ‰¹é‡å¤„ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        
        # è¾“å‡ºè¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
        if HAS_CONNECTION_POOL:
            try:
                stats = get_pool_stats()
                logger.info(f"è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯: {stats}")
            except Exception as e:
                logger.warning(f"è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_items:
        failed_report_file = results_dir / "failed_queries.json"
        with open(failed_report_file, 'w', encoding='utf-8') as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
        logger.info(f"å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_report_file}")
    
    # ä¿å­˜æ¯ä¸ªinstanceçš„ç»“æœ
    instance_results_file = results_dir / "instance_results.json"
    with open(instance_results_file, 'w', encoding='utf-8') as f:
        json.dump(instance_results, f, ensure_ascii=False, indent=2)
    logger.info(f"Instanceç»“æœå·²ä¿å­˜åˆ°: {instance_results_file}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    avg_iterations = total_iterations / success_count if success_count > 0 else 0
    
    summary_report = {
        "total_queries": total_queries,
        "successful": success_count,
        "failed": failed_count,
        "success_rate": f"{success_count/total_queries*100:.2f}%" if total_queries > 0 else "0%",
        "average_iterations": f"{avg_iterations:.2f}",
        "completed_no_data_count": completed_no_data_count,
        "success_definition": "æˆåŠŸå®šä¹‰ï¼šåœ¨è§„å®šæ¬¡æ•°å†…ç”Ÿæˆå‡ºå¯ä»¥æŸ¥è¯¢åˆ°å¤§äºä¸€æ¡æ•°æ®çš„ç»“æœ",
        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "results_directory": str(results_dir)
    }
    
    summary_file = results_dir / "summary_report.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
    
    return summary_report

def main():
    """ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ¨¡å¼"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("å¯åŠ¨SQLç”Ÿæˆç³»ç»Ÿ - æ‰¹é‡å¤„ç†æ¨¡å¼")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='SQLç”Ÿæˆç³»ç»Ÿ - æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--input-file', '-i', type=str, default='spider2-snow-instances-nodata.jsonl', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max-workers', type=int, default=min(16, (os.cpu_count() or 1) + 4), help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--timeout', type=int, default=300, help='å•ä¸ªæŸ¥è¯¢è¶…æ—¶æ—¶é—´(ç§’)')
    
    args = parser.parse_args()
    
    try:
        # è·å–å½“å‰ç›®å½•
        current_dir = Path(__file__).parent
        
        print("SQLç”Ÿæˆç³»ç»Ÿ - æ‰¹é‡å¤„ç†æ¨¡å¼")
        print("="*50)
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        input_file = current_dir / args.input_file
        if not input_file.exists():
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            sys.exit(1)
        
        # åŠ è½½æŸ¥è¯¢æ•°æ®
        queries = load_queries(input_file)
        if not queries:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æŸ¥è¯¢æ•°æ®ï¼Œç¨‹åºé€€å‡º")
            print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æŸ¥è¯¢æ•°æ®")
            sys.exit(1)
        
        print(f"åŠ è½½äº† {len(queries)} ä¸ªæŸ¥è¯¢")
        print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = create_timestamped_directory(current_dir, "batch_results")
        print(f"ç»“æœå°†ä¿å­˜åˆ°: {results_dir}")
        print("\nå¼€å§‹æ‰¹é‡å¤„ç†...")
        
        # ä½¿ç”¨process_batch_querieså‡½æ•°
        start_time = time.time()
        summary_report = process_batch_queries(queries, results_dir, args.max_workers, args.timeout)
        total_time = time.time() - start_time
        
        # æ›´æ–°æ±‡æ€»æŠ¥å‘Šæ·»åŠ æ—¶é—´ä¿¡æ¯
        summary_report.update({
            "max_workers": args.max_workers,
            "timeout_seconds": args.timeout,
            "total_time": f"{total_time:.2f}ç§’",
            "avg_time_per_query": f"{total_time/len(queries):.2f}ç§’"
        })
        
        # é‡æ–°ä¿å­˜æ›´æ–°åçš„æ±‡æ€»æŠ¥å‘Š
        summary_file = results_dir / "summary_report.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        print("\n" + "="*60)
        print("æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š å¤„ç†ç»“æœ")
        print("-" * 60)
        print(f"æ€»æŸ¥è¯¢æ•°: {summary_report['total_queries']}")
        print(f"æˆåŠŸå¤„ç†: {summary_report['successful']} (æœ‰æ•°æ®è¿”å›)")
        print(f"å¤„ç†å¤±è´¥: {summary_report['failed']} (æ— æ•°æ®è¿”å›æˆ–æ‰§è¡Œå¤±è´¥)")
        print(f"æˆåŠŸç‡: {summary_report['success_rate']}")
        print(f"å¹³å‡ä¿®å¤è½®æ•°: {summary_report['average_iterations']}")
        print(f"å®Œæˆä½†æ— æ•°æ®: {summary_report['completed_no_data_count']}")
        print(f"æ€»è€—æ—¶: {summary_report.get('total_time', 'æœªçŸ¥')}")
        print(f"å¹³å‡æ¯ä¸ªæŸ¥è¯¢: {summary_report.get('avg_time_per_query', 'æœªçŸ¥')}")
        print(f"å¹¶å‘çº¿ç¨‹æ•°: {args.max_workers}")
        print(f"ç»“æœç›®å½•: {results_dir}")
        print("="*60)
        
        if summary_report['failed'] > 0:
            print(f"\nâš ï¸  æœ‰ {summary_report['failed']} ä¸ªæŸ¥è¯¢å¤„ç†å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ failed_queries.json")
        
        print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ° instance_results.json")
        
        # æ˜¾ç¤ºè¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯
        if HAS_CONNECTION_POOL:
            try:
                stats = get_pool_stats()
                print(f"\nğŸ”— è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯:")
                print(f"  æ€»åˆ›å»ºè¿æ¥æ•°: {stats.get('total_created', 0)}")
                print(f"  æ€»é”€æ¯è¿æ¥æ•°: {stats.get('total_destroyed', 0)}")
                print(f"  æ€»å€Ÿç”¨è¿æ¥æ•°: {stats.get('total_borrowed', 0)}")
                print(f"  æ€»å½’è¿˜è¿æ¥æ•°: {stats.get('total_returned', 0)}")
                print(f"  æ€»é‡è¯•æ¬¡æ•°: {stats.get('total_retries', 0)}")
                print(f"  å½“å‰è¿æ¥æ± å¤§å°: {stats.get('pool_size', 0)}")
                print(f"  å½“å‰æ´»è·ƒè¿æ¥æ•°: {stats.get('current_active', 0)}")
            except Exception as e:
                print(f"âš ï¸  è·å–è¿æ¥æ± ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        
        # æ¸…ç†è¿æ¥æ± 
        if HAS_CONNECTION_POOL:
            try:
                close_global_pool()
                logger.info("è¿æ¥æ± å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­è¿æ¥æ± å¤±è´¥: {e}")
        
        sys.exit(0)
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        
        # æ¸…ç†è¿æ¥æ± 
        if HAS_CONNECTION_POOL:
            try:
                close_global_pool()
                logger.info("è¿æ¥æ± å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­è¿æ¥æ± å¤±è´¥: {e}")
        
        sys.exit(1)
    
    finally:
        # ç¡®ä¿è¿æ¥æ± è¢«æ­£ç¡®å…³é—­
        if HAS_CONNECTION_POOL:
            try:
                close_global_pool()
                logger.info("è¿æ¥æ± å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­è¿æ¥æ± å¤±è´¥: {e}")

if __name__ == "__main__":
    main()