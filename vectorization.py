import os
import json
import logging
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from dotenv import load_dotenv
import faiss
from openai import OpenAI
from tqdm import tqdm
from utils.CypherExecutor import CypherExecutor


class VectorizedFieldManager:
    def __init__(self, enable_info_logging=True):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å­—æ®µç®¡ç†å™¨
        
        Args:
            enable_info_logging (bool): æ˜¯å¦å¯ç”¨infoçº§åˆ«æ—¥å¿—
        """
        self.enable_info_logging = enable_info_logging
        self.setup_logging()
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv(".env")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        )
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self.cypher_executor = CypherExecutor(enable_info_logging=enable_info_logging)
        
        # è®¾ç½®å‘é‡ç›®å½•
        self.vector_dir = Path("resource/vector")
        self.vector_dir.mkdir(parents=True, exist_ok=True)
        
        # å‘é‡ç»´åº¦
        self.embedding_dim = 1536  # text-embedding-3-smallçš„ç»´åº¦
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # è®¾ç½®å…¨å±€æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œå‡å°‘å™ªéŸ³
        logging.basicConfig(
            level=logging.ERROR,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # å¦‚æœå¯ç”¨è¯¦ç»†æ—¥å¿—ï¼Œåˆ™è®¾ç½®ä¸ºDEBUGçº§åˆ«
        if self.enable_info_logging:
            logging.getLogger().setLevel(logging.DEBUG)
    
    def _log_info(self, message: str):
        """æ¡ä»¶æ€§è®°å½•debugæ—¥å¿—"""
        if self.enable_info_logging:
            logging.debug(message)
    
    def get_database_list(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æ•°æ®åº“åˆ—è¡¨
        
        Returns:
            List[str]: æ•°æ®åº“åç§°åˆ—è¡¨
        """
        cypher_query = """
        MATCH (f:Field)
        RETURN DISTINCT f.database as database
        ORDER BY f.database
        """
        
        success, results = self.cypher_executor.execute_transactional_cypher(cypher_query)
        
        if not success:
            logging.error("Failed to fetch database list")
            return []
        
        databases = [result.get('database', 'UNKNOWN') for result in results if result.get('database')]
        self._log_info(f"Found {len(databases)} databases: {databases}")
        return databases
    
    def get_database_field_count(self, database: str) -> int:
        """
        è·å–æŒ‡å®šæ•°æ®åº“çš„å­—æ®µæ€»æ•°
        
        Args:
            database (str): æ•°æ®åº“åç§°
            
        Returns:
            int: å­—æ®µæ€»æ•°
        """
        cypher_query = """
        MATCH (f:Field)
        WHERE f.database = $database
        RETURN count(f) as total_count
        """
        
        parameters = {"database": database}
        self._log_info(f"Getting field count for database {database}")
        success, results = self.cypher_executor.execute_transactional_cypher(cypher_query, parameters)
        
        if not success or not results:
            logging.error(f"Failed to get field count for database {database}")
            return 0
        
        count = results[0].get('total_count', 0)
        self._log_info(f"Database {database} has {count} fields")
        return count
    
    def get_database_fields_paginated(self, database: str, page_size: int = 100, 
                                    offset: int = 0) -> List[Dict]:
        """
        åˆ†é¡µè·å–æŒ‡å®šæ•°æ®åº“çš„å­—æ®µä¿¡æ¯
        
        Args:
            database (str): æ•°æ®åº“åç§°
            page_size (int): æ¯é¡µå¤§å°ï¼Œé»˜è®¤100
            offset (int): åç§»é‡ï¼Œé»˜è®¤0
            
        Returns:
            List[Dict]: å­—æ®µä¿¡æ¯åˆ—è¡¨
        """
        cypher_query = """
        MATCH (f:Field)
        WHERE f.database = $database
        RETURN elementId(f) as field_id, 
               f.name as field_name, 
               f.type as field_type, 
               f.database as database, 
               f.table as table_name, 
               f.description as description,
               f.schema as schema
        ORDER BY f.table, f.name
        SKIP $offset
        LIMIT $page_size
        """
        
        parameters = {
            "database": database,
            "page_size": page_size,
            "offset": offset
        }
        
        self._log_info(f"Querying fields for database {database}, offset: {offset}, page_size: {page_size}")
        success, results = self.cypher_executor.execute_transactional_cypher(cypher_query, parameters)
        
        if not success:
            logging.error(f"Failed to fetch fields for database {database} (offset: {offset})")
            return []
        
        fields = []
        for result in results:
            field_info = {
                'id': result.get('field_id'),
                'name': result.get('field_name'),
                'type': result.get('field_type'),
                'database': result.get('database'),
                'table': result.get('table_name'),
                'description': result.get('description', ''),
                'schema': result.get('schema', '')
            }
            fields.append(field_info)
        
        return fields
    
    def get_all_database_fields(self, database: str, page_size: int = 100, 
                              show_progress: bool = True) -> List[Dict]:
        """
        è·å–æŒ‡å®šæ•°æ®åº“çš„æ‰€æœ‰å­—æ®µï¼Œåˆ†é¡µåŠ è½½å¹¶æ˜¾ç¤ºè¿›åº¦
        
        Args:
            database (str): æ•°æ®åº“åç§°
            page_size (int): æ¯é¡µå¤§å°ï¼Œé»˜è®¤100
            show_progress (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼Œé»˜è®¤True
            
        Returns:
            List[Dict]: å­—æ®µä¿¡æ¯åˆ—è¡¨
        """
        # è·å–æ€»å­—æ®µæ•°
        total_count = self.get_database_field_count(database)
        if total_count == 0:
            self._log_info(f"Database {database} has no fields")
            return []
        
        all_fields = []
        offset = 0
        
        # è®¡ç®—æ€»é¡µæ•°
        total_pages = (total_count + page_size - 1) // page_size
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºåˆ†é¡µè¿›åº¦
        page_iter = range(total_pages)
        if show_progress:
            page_iter = tqdm(page_iter, desc=f"åŠ è½½ {database} å­—æ®µ", unit="é¡µ")
        
        for page_num in page_iter:
            # è·å–å½“å‰é¡µçš„å­—æ®µ
            fields = self.get_database_fields_paginated(database, page_size, offset)
            
            if not fields:
                logging.warning(f"No fields returned for database {database} at offset {offset}")
                break
            
            all_fields.extend(fields)
            offset += page_size
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            if show_progress:
                page_iter.set_postfix({
                    'å·²åŠ è½½': len(all_fields),
                    'æ€»è®¡': total_count
                })
        
        self._log_info(f"Successfully loaded {len(all_fields)} fields for database {database}")
        return all_fields
    
    def get_all_field_nodes(self, target_databases: List[str] = None, 
                          page_size: int = 100, show_progress: bool = True) -> Dict[str, List[Dict]]:
        """
        åˆ†é¡µè·å–FieldèŠ‚ç‚¹ï¼ŒæŒ‰databaseåˆ†ç±»ï¼Œæ”¯æŒè¿›åº¦æ˜¾ç¤º
        
        Args:
            target_databases (List[str], optional): ç›®æ ‡æ•°æ®åº“åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰æ•°æ®åº“
            page_size (int): æ¯é¡µå¤§å°ï¼Œé»˜è®¤100
            show_progress (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼Œé»˜è®¤True
            
        Returns:
            Dict[str, List[Dict]]: æŒ‰æ•°æ®åº“åˆ†ç±»çš„å­—æ®µä¿¡æ¯
        """
        # è·å–æ•°æ®åº“åˆ—è¡¨
        if target_databases is None:
            databases = self.get_database_list()
        else:
            databases = target_databases
        
        if not databases:
            logging.error("No databases found")
            return {}
        
        if show_progress:
            print(f"å¼€å§‹è·å– {len(databases)} ä¸ªæ•°æ®åº“çš„å­—æ®µä¿¡æ¯...")
        
        fields_by_database = {}
        
        for i, database in enumerate(databases, 1):
            if show_progress:
                print(f"\n[{i}/{len(databases)}] å¤„ç†æ•°æ®åº“: {database}")
            
            try:
                fields = self.get_all_database_fields(database, page_size, show_progress)
                fields_by_database[database] = fields
                
                if show_progress:
                    print(f"  âœ… å®Œæˆæ•°æ®åº“ {database}: {len(fields)} ä¸ªå­—æ®µ")
                    
            except Exception as e:
                logging.error(f"Failed to load fields for database {database}: {e}")
                fields_by_database[database] = []
                if show_progress:
                    print(f"  âŒ æ•°æ®åº“ {database} åŠ è½½å¤±è´¥: {e}")
        
        total_fields = sum(len(fields) for fields in fields_by_database.values())
        self._log_info(f"Retrieved {total_fields} fields from {len(fields_by_database)} databases")
        
        if show_progress:
            print(f"\n=== è·å–å®Œæˆ ===")
            print(f"æ€»è®¡: {total_fields} ä¸ªå­—æ®µï¼Œæ¥è‡ª {len(fields_by_database)} ä¸ªæ•°æ®åº“")
        
        return fields_by_database
    
    def format_field_for_vectorization(self, field_info: Dict) -> str:
        """
        å°†å­—æ®µä¿¡æ¯æ ¼å¼åŒ–ä¸ºå‘é‡åŒ–æ–‡æœ¬
        
        Args:
            field_info (Dict): å­—æ®µä¿¡æ¯
            
        Returns:
            str: æ ¼å¼åŒ–çš„å‘é‡åŒ–æ–‡æœ¬
        """
        field_name = field_info.get('name', 'unknown')
        field_type = field_info.get('type', 'unknown')
        table_name = field_info.get('table', 'unknown')
        database = field_info.get('database', 'unknown')
        schema = field_info.get('schema', '')
        description = field_info.get('description', '').strip()
        
        # æ„å»ºå®Œæ•´çš„è¡¨åï¼ˆåŒ…å«schemaï¼‰
        if schema and schema.strip():
            full_table_name = f"{schema}.{table_name}"
        else:
            full_table_name = table_name
        
        if description:
            desc_text = f"Description: {description}"
        else:
            desc_text = "No description available."
        
        return f"Field {field_name} (type: {field_type}), from table {full_table_name} in database {database}. {desc_text}"
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        ä½¿ç”¨OpenAI APIè·å–æ–‡æœ¬å‘é‡
        
        Args:
            texts (List[str]): å¾…å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[List[float]]: å‘é‡åˆ—è¡¨
        """
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=texts
            )
            
            embeddings = [data.embedding for data in response.data]
            self._log_info(f"Generated embeddings for {len(texts)} texts")
            return embeddings
            
        except Exception as e:
            logging.error(f"Failed to generate embeddings: {e}")
            return []
    
    def build_faiss_index(self, embeddings: List[List[float]]) -> faiss.IndexFlatIP:
        """
        æ„å»ºFAISSç´¢å¼•
        
        Args:
            embeddings (List[List[float]]): å‘é‡åˆ—è¡¨
            
        Returns:
            faiss.IndexFlatIP: FAISSç´¢å¼•
        """
        embeddings_array = np.array(embeddings, dtype=np.float32)
        
        # å½’ä¸€åŒ–å‘é‡ä»¥ä½¿ç”¨å†…ç§¯è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        faiss.normalize_L2(embeddings_array)
        
        # åˆ›å»ºç´¢å¼•
        index = faiss.IndexFlatIP(self.embedding_dim)
        index.add(embeddings_array)
        
        self._log_info(f"Built FAISS index with {index.ntotal} vectors")
        return index
    
    def save_database_vectors(self, database: str, page_size: int = 100, 
                            embedding_batch_size: int = 50, show_progress: bool = True) -> bool:
        """
        ä¸ºå•ä¸ªæ•°æ®åº“ä¿å­˜å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®ï¼Œæ”¯æŒåˆ†é¡µå’Œæ‰¹é‡å¤„ç†
        
        Args:
            database (str): æ•°æ®åº“åç§°
            page_size (int): å­—æ®µåˆ†é¡µå¤§å°ï¼Œé»˜è®¤100
            embedding_batch_size (int): å‘é‡åŒ–æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50
            show_progress (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼Œé»˜è®¤True
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        # åˆ†é¡µè·å–å­—æ®µ
        fields = self.get_all_database_fields(database, page_size, show_progress)
        
        if not fields:
            self._log_info(f"No fields found for database {database}")
            return True
        
        self._log_info(f"Processing {len(fields)} fields for database {database}")
        
        # åˆ†æ‰¹å¤„ç†å‘é‡åŒ–ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
        all_embeddings = []
        all_texts = []
        
        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (len(fields) + embedding_batch_size - 1) // embedding_batch_size
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºå‘é‡åŒ–è¿›åº¦
        batch_range = range(0, len(fields), embedding_batch_size)
        if show_progress:
            batch_range = tqdm(batch_range, desc=f"å‘é‡åŒ– {database}", unit="æ‰¹æ¬¡", total=total_batches)
        
        for i in batch_range:
            batch_fields = fields[i:i + embedding_batch_size]
            batch_texts = [self.format_field_for_vectorization(field) for field in batch_fields]
            
            # è·å–å½“å‰æ‰¹æ¬¡çš„å‘é‡
            batch_embeddings = self.get_embeddings(batch_texts)
            if not batch_embeddings:
                batch_num = i // embedding_batch_size + 1
                logging.error(f"Failed to generate embeddings for batch {batch_num} of database {database}")
                return False
            
            all_embeddings.extend(batch_embeddings)
            all_texts.extend(batch_texts)
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            if show_progress:
                batch_range.set_postfix({
                    'å·²å¤„ç†': len(all_embeddings),
                    'æ€»è®¡': len(fields)
                })
        
        # æ„å»ºç´¢å¼•
        index = self.build_faiss_index(all_embeddings)
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        index_path = self.vector_dir / f"faiss_index_{database}.bin"
        faiss.write_index(index, str(index_path))
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = self.vector_dir / f"metadata_{database}.jsonl"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            for i, field in enumerate(fields):
                metadata = {
                    'vector_index': i,
                    'field_id': field['id'],
                    'field_name': field['name'],
                    'field_type': field['type'],
                    'table': field['table'],
                    'database': field['database'],
                    'schema': field.get('schema', ''),
                    'description': field['description'],
                    'vectorization_text': all_texts[i]
                }
                f.write(json.dumps(metadata, ensure_ascii=False) + '\n')
        
        self._log_info(f"Successfully saved vectors for database {database}")
        return True
    
    def vectorize_database(self, database_name: str, page_size: int = 100, 
                         embedding_batch_size: int = 50, show_progress: bool = True) -> bool:
        """
        ä¸ºæŒ‡å®šæ•°æ®åº“ç”Ÿæˆå‘é‡ç´¢å¼•
        
        Args:
            database_name (str): æ•°æ®åº“åç§°
            page_size (int): å­—æ®µåˆ†é¡µå¤§å°ï¼Œé»˜è®¤100
            embedding_batch_size (int): å‘é‡åŒ–æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50
            show_progress (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼Œé»˜è®¤True
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self._log_info(f"Starting vectorization for database: {database_name}")
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        databases = self.get_database_list()
        if database_name not in databases:
            logging.error(f"Database {database_name} not found. Available databases: {databases}")
            return False
        
        return self.save_database_vectors(database_name, page_size, embedding_batch_size, show_progress)
    
    def vectorize_all_databases(self, page_size: int = 100, embedding_batch_size: int = 50, 
                              show_progress: bool = True) -> bool:
        """
        ä¸ºæ‰€æœ‰æ•°æ®åº“ç”Ÿæˆå‘é‡ç´¢å¼•
        
        Args:
            page_size (int): å­—æ®µåˆ†é¡µå¤§å°ï¼Œé»˜è®¤100
            embedding_batch_size (int): å‘é‡åŒ–æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50
            show_progress (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼Œé»˜è®¤True
        
        Returns:
            bool: æ˜¯å¦å…¨éƒ¨æˆåŠŸ
        """
        self._log_info("Starting vectorization for all databases")
        
        # è·å–æ•°æ®åº“åˆ—è¡¨
        databases = self.get_database_list()
        
        if not databases:
            logging.error("No databases found")
            return False
        
        success_count = 0
        total_count = len(databases)
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ•°æ®åº“å¤„ç†è¿›åº¦
        db_iter = databases
        if show_progress:
            db_iter = tqdm(databases, desc="å…¨é‡å‘é‡åŒ–", unit="æ•°æ®åº“")
        
        for database in db_iter:
            try:
                if self.save_database_vectors(database, page_size, embedding_batch_size, show_progress):
                    success_count += 1
                    if show_progress:
                        db_iter.set_postfix({
                            'æˆåŠŸ': success_count,
                            'å½“å‰': database[:15] + '...' if len(database) > 15 else database
                        })
                else:
                    logging.error(f"Failed to vectorize database: {database}")
            except Exception as e:
                logging.error(f"Exception during vectorization of database {database}: {e}")
        
        if show_progress:
            print(f"\nå…¨é‡å‘é‡åŒ–å®Œæˆ: {success_count}/{total_count} ä¸ªæ•°æ®åº“æˆåŠŸ")
        
        self._log_info(f"Vectorization completed: {success_count}/{total_count} databases successful")
        return success_count == total_count
    
    def load_database_index(self, database: str) -> Tuple[Optional[faiss.IndexFlatIP], Optional[List[Dict]]]:
        """
        åŠ è½½æŒ‡å®šæ•°æ®åº“çš„å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®
        
        Args:
            database (str): æ•°æ®åº“åç§°
            
        Returns:
            Tuple[Optional[faiss.IndexFlatIP], Optional[List[Dict]]]: ç´¢å¼•å’Œå…ƒæ•°æ®
        """
        index_path = self.vector_dir / f"faiss_index_{database}.bin"
        metadata_path = self.vector_dir / f"metadata_{database}.jsonl"
        
        if not index_path.exists() or not metadata_path.exists():
            logging.error(f"Index or metadata file not found for database {database}")
            return None, None
        
        try:
            # åŠ è½½ç´¢å¼•
            index = faiss.read_index(str(index_path))
            
            # åŠ è½½å…ƒæ•°æ®
            metadata = []
            with open(metadata_path, 'r', encoding='utf-8') as f:
                for line in f:
                    metadata.append(json.loads(line.strip()))
            
            self._log_info(f"Loaded index for database {database}: {index.ntotal} vectors")
            return index, metadata
            
        except Exception as e:
            logging.error(f"Failed to load index for database {database}: {e}")
            return None, None
    
    def search_fields(self, query: str, database: str, top_k: int = 5) -> List[Dict]:
        """
        åœ¨æŒ‡å®šæ•°æ®åº“ä¸­æœç´¢ç›¸å…³å­—æ®µ
        
        Args:
            query (str): æŸ¥è¯¢æ–‡æœ¬
            database (str): æ•°æ®åº“åç§°
            top_k (int): è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        # åŠ è½½ç´¢å¼•
        index, metadata = self.load_database_index(database)
        if index is None or metadata is None:
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_embeddings = self.get_embeddings([query])
        if not query_embeddings:
            logging.error("Failed to generate query embedding")
            return []
        
        query_vector = np.array([query_embeddings[0]], dtype=np.float32)
        faiss.normalize_L2(query_vector)
        
        # æœç´¢
        scores, indices = index.search(query_vector, min(top_k, len(metadata)))
        
        # æ ¼å¼åŒ–ç»“æœ
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1:  # æœ‰æ•ˆç´¢å¼•
                result = metadata[idx].copy()
                result['similarity_score'] = float(score)
                result['rank'] = i + 1
                results.append(result)
        
        self._log_info(f"Found {len(results)} results for query in database {database}")
        return results
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.cypher_executor:
            self.cypher_executor.close()


def test():
    """æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•°"""
    print("=== å­—æ®µå‘é‡åŒ–ç³»ç»Ÿæµ‹è¯• ===\n")
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = VectorizedFieldManager(enable_info_logging=True)
    
    try:
        # éªŒè¯æ•°æ®åº“è¿æ¥
        print("1. éªŒè¯æ•°æ®åº“è¿æ¥...")
        if not manager.cypher_executor.verify_connectivity():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
        
        # è·å–æ•°æ®åº“åˆ—è¡¨
        print("2. è·å–æ•°æ®åº“åˆ—è¡¨...")
        databases = manager.get_database_list()
        
        if not databases:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®åº“")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(databases)} ä¸ªæ•°æ®åº“: {databases}")
        
        # é€‰æ‹©ä¸€ä¸ªæ•°æ®åº“è¿›è¡Œæµ‹è¯•ï¼ˆä¼˜å…ˆé€‰æ‹©CRYPTOï¼Œå¦‚æœæ²¡æœ‰åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ªï¼‰
        test_database = "CRYPTO" if "CRYPTO" in databases else databases[0]
        
        print(f"\n3. æµ‹è¯•å­—æ®µè·å–å’Œå‘é‡åŒ–æ ¼å¼ï¼ˆä½¿ç”¨æ•°æ®åº“: {test_database}ï¼‰...")
        # è·å–å°‘é‡å­—æ®µç”¨äºæ ¼å¼æµ‹è¯•
        test_fields = manager.get_database_fields_paginated(test_database, page_size=3)
        
        if test_fields:
            for field in test_fields:
                formatted_text = manager.format_field_for_vectorization(field)
                print(f"   å­—æ®µID: {field['id']}")
                print(f"   å‘é‡åŒ–æ–‡æœ¬: {formatted_text}")
                print()
        else:
            print(f"   æ•°æ®åº“ {test_database} ä¸­æ²¡æœ‰æ‰¾åˆ°å­—æ®µ")
        
        # æµ‹è¯•å•æ•°æ®åº“å‘é‡åŒ–
        print(f"4. æµ‹è¯•å•æ•°æ®åº“å‘é‡åŒ–ï¼ˆ{test_database}ï¼‰...")
        if manager.vectorize_database(test_database, page_size=50, embedding_batch_size=10):
            print(f"âœ… æ•°æ®åº“ {test_database} å‘é‡åŒ–æˆåŠŸ")
        else:
            print(f"âŒ æ•°æ®åº“ {test_database} å‘é‡åŒ–å¤±è´¥")
            return
        print()
        
        # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
        print("5. æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        test_queries = [
            "user information",
            "timestamp field", 
            "transaction data",
            "block data",
            "address"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: '{query}'")
            results = manager.search_fields(query, test_database, top_k=3)
            
            if results:
                print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³å­—æ®µ:")
                for result in results:
                    print(f"  {result['rank']}. {result['field_name']} ({result['field_type']})")
                    print(f"     è¡¨: {result['table']}")
                    print(f"     ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
            else:
                print("  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å­—æ®µ")
        
        print("\n=== æµ‹è¯•å®Œæˆ ===")
        
    except Exception as e:
        logging.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    finally:
        # å…³é—­è¿æ¥
        manager.close()


if __name__ == "__main__":
    # test()  # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œæµ‹è¯•
    
    # åˆå§‹åŒ–ç®¡ç†å™¨ - å…³é—­è¯¦ç»†æ—¥å¿—ä»¥å‡å°‘è¾“å‡º
    manager = VectorizedFieldManager(enable_info_logging=False)
    
    try:
        print("å¼€å§‹å…¨é‡å‘é‡åŒ–...")
        print("æ³¨æ„: è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
        print("=" * 50)
        
        if manager.vectorize_all_databases(page_size=200, embedding_batch_size=50):
            print("\nğŸ‰ å…¨é‡å‘é‡åŒ–æˆåŠŸå®Œæˆ!")
        else:
            print("\nâŒ å…¨é‡å‘é‡åŒ–å¤±è´¥")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å…¨é‡å‘é‡åŒ–å¤±è´¥: {e}")
        logging.error(f"Vectorization failed with exception: {e}")
    finally:
        manager.close()
        print("è¿æ¥å·²å…³é—­")
